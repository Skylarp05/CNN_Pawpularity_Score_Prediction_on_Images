{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":25383,"databundleVersionId":2684322,"sourceType":"competition"},{"sourceId":2912343,"sourceType":"datasetVersion","datasetId":1785124},{"sourceId":2912396,"sourceType":"datasetVersion","datasetId":1785154},{"sourceId":2912434,"sourceType":"datasetVersion","datasetId":1785181},{"sourceId":2983986,"sourceType":"datasetVersion","datasetId":1826191},{"sourceId":2984144,"sourceType":"datasetVersion","datasetId":1826396},{"sourceId":3020522,"sourceType":"datasetVersion","datasetId":1843173},{"sourceId":3024588,"sourceType":"datasetVersion","datasetId":1852388},{"sourceId":3035406,"sourceType":"datasetVersion","datasetId":1859021},{"sourceId":3035420,"sourceType":"datasetVersion","datasetId":1845210},{"sourceId":3040753,"sourceType":"datasetVersion","datasetId":1862338},{"sourceId":3047700,"sourceType":"datasetVersion","datasetId":1700843},{"sourceId":3852749,"sourceType":"datasetVersion","datasetId":1741670},{"sourceId":3951115,"sourceType":"datasetVersion","datasetId":1027206},{"sourceId":80778021,"sourceType":"kernelVersion"}],"dockerImageVersionId":30146,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\n\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport timm\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nimport pandas as pd\nimport numpy as np\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nimport joblib\nimport gc\nfrom glob import glob\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\nimport timm\nfrom PIL import Image\nimport PIL\n\nfrom tqdm import tqdm\nimport joblib\nimport time\nfrom tqdm.notebook import tqdm\nimport joblib\nfrom sklearn.model_selection import StratifiedKFold\n\nimport cuml\n\nprint(np.__version__)\nprint(pd.__version__)\nprint(torch.__version__)\nprint(timm.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-05T03:43:04.793655Z","iopub.execute_input":"2023-12-05T03:43:04.794012Z","iopub.status.idle":"2023-12-05T03:43:16.644066Z","shell.execute_reply.started":"2023-12-05T03:43:04.793914Z","shell.execute_reply":"2023-12-05T03:43:16.643232Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"1.19.5\n1.3.4\n1.9.1\n0.6.5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Install OpenAI CLIP","metadata":{}},{"cell_type":"code","source":"!pip install ../input/openaiclipweights/python-ftfy-master/python-ftfy-master\n!pip install ../input/openaiclipweights/clip/CLIP\n!cp ../input/openaiclipweights/CLIP-main/CLIP-main/clip/bpe_simple_vocab_16e6.txt /opt/conda/lib/python3.7/site-packages/clip/.\n!gzip -k /opt/conda/lib/python3.7/site-packages/clip/bpe_simple_vocab_16e6.txt\n\nimport clip","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:44:01.219026Z","iopub.execute_input":"2023-12-05T03:44:01.219867Z","iopub.status.idle":"2023-12-05T03:45:25.514469Z","shell.execute_reply.started":"2023-12-05T03:44:01.219811Z","shell.execute_reply":"2023-12-05T03:45:25.513663Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/openaiclipweights/python-ftfy-master/python-ftfy-master\n\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from ftfy==6.0.2) (0.2.5)\nBuilding wheels for collected packages: ftfy\n  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ftfy: filename=ftfy-6.0.2-py3-none-any.whl size=41932 sha256=ae6fa474ff0490aa847c04d05054e9f91ca55c0c40b5b22d4fc2b63b3bfae62a\n  Stored in directory: /root/.cache/pip/wheels/2e/98/80/ce5445e83c31c177bd363169ed65e9cb14156696de5721bd36\nSuccessfully built ftfy\nInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nProcessing /kaggle/input/openaiclipweights/clip/CLIP\n\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\nRequirement already satisfied: ftfy in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (6.0.2)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (2021.8.28)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (4.62.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (1.9.1)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (0.10.1)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from ftfy->clip==1.0) (0.2.5)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->clip==1.0) (3.10.0.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (1.19.5)\nRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (8.2.0)\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=11550 sha256=d72167e6acf4e497eb5e3e5ab59e9362608264afe712faf4521a50da0ebdc395\n  Stored in directory: /tmp/pip-ephem-wheel-cache-muytwf26/wheels/f7/2f/29/2a4720b63c9fb3010137dd6a398f5c8ab41f1eb440ac7dda40\nSuccessfully built clip\nInstalling collected packages: clip\nSuccessfully installed clip-1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Train and Test","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/petfinderdata/train-folds-1.csv')\ntest = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nsub = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')\n\ntrain['path'] = train['Id'].map(lambda x: '../input/petfinder-pawpularity-score/train/'+x+'.jpg')\ntest['path'] = test['Id'].map(lambda x: '../input/petfinder-pawpularity-score/test/'+x+'.jpg')\n\n# If its Public LB run, then augment Testset to chack batch size memory consumption.\nif test.shape[0]<10:\n    test = pd.concat([\n        test, test, test, test, test, \n    ])\n    test = test.reset_index(drop=True)\n\nprint(train.shape, test.shape, sub.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:45:50.258941Z","iopub.execute_input":"2023-12-05T03:45:50.259740Z","iopub.status.idle":"2023-12-05T03:45:50.338054Z","shell.execute_reply.started":"2023-12-05T03:45:50.259695Z","shell.execute_reply":"2023-12-05T03:45:50.337379Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(9912, 18) (40, 14) (8, 2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Just create K Folds and check target consistency accross folds","metadata":{}},{"cell_type":"code","source":"train['bins'] = (train['Pawpularity']//5).round()\n\ntrain['fold0'] = -1\nskf = StratifiedKFold(n_splits = 20, shuffle=True, random_state = 1)\nfor i, (_, test_index) in enumerate(skf.split(train.index, train['bins'])):\n    train.iloc[test_index, -1] = i\n\ntrain['fold0'] = train['fold0'].astype('int')\ngc.collect()\n\ntrain.groupby(['fold0'])['Pawpularity'].agg(['mean','std','count'])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:45:59.851150Z","iopub.execute_input":"2023-12-05T03:45:59.851436Z","iopub.status.idle":"2023-12-05T03:46:00.163343Z","shell.execute_reply.started":"2023-12-05T03:45:59.851403Z","shell.execute_reply":"2023-12-05T03:46:00.162609Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"            mean        std  count\nfold0                             \n0      38.044355  20.623269    496\n1      38.016129  20.599788    496\n2      38.018145  20.700209    496\n3      38.028226  20.665865    496\n4      38.026210  20.756338    496\n5      38.016129  20.668127    496\n6      38.070565  20.590600    496\n7      38.030242  20.731401    496\n8      38.125000  20.565454    496\n9      38.120968  20.697179    496\n10     38.064516  20.622334    496\n11     38.137097  20.657802    496\n12     37.941414  20.470741    495\n13     37.923232  20.480023    495\n14     37.991919  20.599908    495\n15     38.098990  20.593234    495\n16     37.995960  20.520454    495\n17     38.060606  20.500527    495\n18     38.014141  20.590125    495\n19     38.056566  20.596294    495","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>std</th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>fold0</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>38.044355</td>\n      <td>20.623269</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38.016129</td>\n      <td>20.599788</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38.018145</td>\n      <td>20.700209</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38.028226</td>\n      <td>20.665865</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>38.026210</td>\n      <td>20.756338</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>38.016129</td>\n      <td>20.668127</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>38.070565</td>\n      <td>20.590600</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>38.030242</td>\n      <td>20.731401</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>38.125000</td>\n      <td>20.565454</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>38.120968</td>\n      <td>20.697179</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>38.064516</td>\n      <td>20.622334</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>38.137097</td>\n      <td>20.657802</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>37.941414</td>\n      <td>20.470741</td>\n      <td>495</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>37.923232</td>\n      <td>20.480023</td>\n      <td>495</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>37.991919</td>\n      <td>20.599908</td>\n      <td>495</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>38.098990</td>\n      <td>20.593234</td>\n      <td>495</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>37.995960</td>\n      <td>20.520454</td>\n      <td>495</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>38.060606</td>\n      <td>20.500527</td>\n      <td>495</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>38.014141</td>\n      <td>20.590125</td>\n      <td>495</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>38.056566</td>\n      <td>20.596294</td>\n      <td>495</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:46:01.679262Z","iopub.execute_input":"2023-12-05T03:46:01.679575Z","iopub.status.idle":"2023-12-05T03:46:01.697101Z","shell.execute_reply.started":"2023-12-05T03:46:01.679524Z","shell.execute_reply":"2023-12-05T03:46:01.696345Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n0  3ed899a8334a8e5c74f4a554f3ce6f08              0     1     1     1       0   \n1  e0a1efdaf4fbed8659b6d23994ee346e              0     1     1     1       0   \n2  6c159aede3df25fdbe781431aabcfc67              0     1     1     1       0   \n3  53b536999aecd800cfda720f3ca363cb              0     1     1     1       0   \n4  4e3c8816d95b083b870c6747a26fcb58              0     0     0     1       0   \n\n   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  fold  \\\n0          0      0        0      0          0     0     0            1     1   \n1          0      0        0      1          1     0     0            1     2   \n2          0      0        0      0          0     0     0            1     3   \n3          0      0        0      0          0     0     0            1     4   \n4          0      0        0      1          1     0     1            2     5   \n\n   dup                               nn0  \\\n0   -1  89a0b49aad4ce34c978b9a9c74cc95bd   \n1   -2  b89199007b4fbd51864eeaf8f264f946   \n2   -3  59b3e4a3c4fe0f1f56cb7741100e7a30   \n3   -4  0a05c55ca864b667d31c80ce2c68d6b3   \n4   -5  7af48bb7190c236d8876fe529b449c3f   \n\n                                                path  bins  fold0  \n0  ../input/petfinder-pawpularity-score/train/3ed...     0     17  \n1  ../input/petfinder-pawpularity-score/train/e0a...     0     14  \n2  ../input/petfinder-pawpularity-score/train/6c1...     0     15  \n3  ../input/petfinder-pawpularity-score/train/53b...     0     19  \n4  ../input/petfinder-pawpularity-score/train/4e3...     0      5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Subject Focus</th>\n      <th>Eyes</th>\n      <th>Face</th>\n      <th>Near</th>\n      <th>Action</th>\n      <th>Accessory</th>\n      <th>Group</th>\n      <th>Collage</th>\n      <th>Human</th>\n      <th>Occlusion</th>\n      <th>Info</th>\n      <th>Blur</th>\n      <th>Pawpularity</th>\n      <th>fold</th>\n      <th>dup</th>\n      <th>nn0</th>\n      <th>path</th>\n      <th>bins</th>\n      <th>fold0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3ed899a8334a8e5c74f4a554f3ce6f08</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>89a0b49aad4ce34c978b9a9c74cc95bd</td>\n      <td>../input/petfinder-pawpularity-score/train/3ed...</td>\n      <td>0</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>e0a1efdaf4fbed8659b6d23994ee346e</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>-2</td>\n      <td>b89199007b4fbd51864eeaf8f264f946</td>\n      <td>../input/petfinder-pawpularity-score/train/e0a...</td>\n      <td>0</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6c159aede3df25fdbe781431aabcfc67</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>-3</td>\n      <td>59b3e4a3c4fe0f1f56cb7741100e7a30</td>\n      <td>../input/petfinder-pawpularity-score/train/6c1...</td>\n      <td>0</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53b536999aecd800cfda720f3ca363cb</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>-4</td>\n      <td>0a05c55ca864b667d31c80ce2c68d6b3</td>\n      <td>../input/petfinder-pawpularity-score/train/53b...</td>\n      <td>0</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4e3c8816d95b083b870c6747a26fcb58</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5</td>\n      <td>-5</td>\n      <td>7af48bb7190c236d8876fe529b449c3f</td>\n      <td>../input/petfinder-pawpularity-score/train/4e3...</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:46:02.283009Z","iopub.execute_input":"2023-12-05T03:46:02.283309Z","iopub.status.idle":"2023-12-05T03:46:02.297209Z","shell.execute_reply.started":"2023-12-05T03:46:02.283266Z","shell.execute_reply":"2023-12-05T03:46:02.296434Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n0  4128bae22183829d2b5fea10effdb0c3              1     0     1     0       0   \n1  43a2262d7738e3d420d453815151079e              0     1     0     0       0   \n2  4e429cead1848a298432a0acad014c9d              0     0     0     1       0   \n3  80bc3ccafcc51b66303c2c263aa38486              1     0     1     0       0   \n4  8f49844c382931444e68dffbe20228f4              1     1     1     0       1   \n\n   Accessory  Group  Collage  Human  Occlusion  Info  Blur  \\\n0          1      1        0      0          1     0     1   \n1          0      1        1      0          0     0     0   \n2          1      1        1      0          1     1     1   \n3          0      0        0      0          0     1     0   \n4          1      0        1      0          1     1     0   \n\n                                                path  \n0  ../input/petfinder-pawpularity-score/test/4128...  \n1  ../input/petfinder-pawpularity-score/test/43a2...  \n2  ../input/petfinder-pawpularity-score/test/4e42...  \n3  ../input/petfinder-pawpularity-score/test/80bc...  \n4  ../input/petfinder-pawpularity-score/test/8f49...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Subject Focus</th>\n      <th>Eyes</th>\n      <th>Face</th>\n      <th>Near</th>\n      <th>Action</th>\n      <th>Accessory</th>\n      <th>Group</th>\n      <th>Collage</th>\n      <th>Human</th>\n      <th>Occlusion</th>\n      <th>Info</th>\n      <th>Blur</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4128bae22183829d2b5fea10effdb0c3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>../input/petfinder-pawpularity-score/test/4128...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>43a2262d7738e3d420d453815151079e</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>../input/petfinder-pawpularity-score/test/43a2...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4e429cead1848a298432a0acad014c9d</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>../input/petfinder-pawpularity-score/test/4e42...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80bc3ccafcc51b66303c2c263aa38486</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>../input/petfinder-pawpularity-score/test/80bc...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8f49844c382931444e68dffbe20228f4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>../input/petfinder-pawpularity-score/test/8f49...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Lest check all models available in timm library","metadata":{}},{"cell_type":"code","source":"avail_pretrained_models = timm.list_models(pretrained=True)\nlen(avail_pretrained_models)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:46:07.135564Z","iopub.execute_input":"2023-12-05T03:46:07.136343Z","iopub.status.idle":"2023-12-05T03:46:07.146197Z","shell.execute_reply.started":"2023-12-05T03:46:07.136305Z","shell.execute_reply":"2023-12-05T03:46:07.145412Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"709"},"metadata":{}}]},{"cell_type":"markdown","source":"As we can see there are 709 pretrained model architectures available in timm library. Most of the models in timm are trained using 1000 classes in imagenet, so output shape is 1000 for each model.\n\nThe first part of the solution is basically extract the features from the last layer of that models and run a SVR on that extracted features. So the idea is to find a subset of models (from that 709) that performs well in terms of RMSE. To do that it was used a forward models selection algorithm, following by RMSE hill climbing logic. Starting with one model, then keep adding models until it stop increasing RMSE performance.\n\nThe pretrained models found by the forward model selection algorithm used in this notebook are listed below.","metadata":{}},{"cell_type":"code","source":"names = [\n    'deit_base_distilled_patch16_384',\n    'fbnetc_100',\n    'ig_resnext101_32x8d',\n    'ig_resnext101_32x48d',\n    'repvgg_b0',\n    'resnetv2_152x4_bitm',\n    'rexnet_200',\n    'resnest269e',\n    'swsl_resnext101_32x8d',\n    'tf_efficientnet_b6_ns',\n    'tf_efficientnet_b7_ns',\n    'tf_efficientnet_b8_ap',\n    'tf_efficientnet_l2_ns_475',\n    'vit_base_patch16_384',\n    'vit_large_patch16_384',\n    'vit_large_r50_s32_384',\n]\n\nnames_hflip_crop = [\n    'tf_efficientnet_l2_ns_hflip_384',\n    'deit_base_distilled_patch16_384_hflip_384',\n    'ig_resnext101_32x48d_hflip_384',\n    'tf_efficientnet_l2_ns_512',\n]\n\nnames_orig = [\n    'ig_resnext101_32x48d',\n    'vit_large_r50_s32_384',\n    'clip_RN50x4',\n    'clip_ViT-B-16',\n    'clip_RN50x16',\n    'clip_ViT-B-32',\n]","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:46:08.833750Z","iopub.execute_input":"2023-12-05T03:46:08.834534Z","iopub.status.idle":"2023-12-05T03:46:08.839945Z","shell.execute_reply.started":"2023-12-05T03:46:08.834493Z","shell.execute_reply":"2023-12-05T03:46:08.839333Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Create a dictionary with the path of all pretrained weights available in Kaggle datasets","metadata":{}},{"cell_type":"code","source":"modelpath = { m.split('/')[-1].split('.')[0] :m for m in glob('../input/pytorch-pretrained-0/*.pt')+glob('../input/pytorch-pretrained-1/*.pt')+glob('../input/pytorch-pretrained-2/*.pt')+glob('../input/pytorch-pretrained-3/*.pt')}\n# modelpath","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:46:10.681607Z","iopub.execute_input":"2023-12-05T03:46:10.682352Z","iopub.status.idle":"2023-12-05T03:46:10.700344Z","shell.execute_reply.started":"2023-12-05T03:46:10.682316Z","shell.execute_reply":"2023-12-05T03:46:10.699774Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Now interactively extract the TESTSET features from each imagenet pretrained model and append to a dictionary","metadata":{}},{"cell_type":"code","source":"class PawpularDataset:\n    def __init__(self, images, base_path='../input/petfinder-pawpularity-score/train/', modelcfg=None, aug=0 ):\n        \n        self.images = images.copy()\n        self.base_path = base_path\n        self.transform = create_transform(**modelcfg)\n        self.aug=aug\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, item):\n        img = Image.open(self.base_path + self.images[item] + '.jpg').convert('RGB')\n        img = self.transform(img)\n        return img\n\n\nEMB_TEST = {}\nfor arch in names:\n    starttime = time.time()\n\n    model = timm.create_model(arch, pretrained=False).to('cuda')\n    model.load_state_dict(torch.load(modelpath[arch]))\n    model.eval()\n\n    train_dataset = PawpularDataset(\n        images = test.Id.values,\n        base_path='../input/petfinder-pawpularity-score/test/',\n        modelcfg = resolve_data_config({}, model=model),\n        aug = 0,\n    )\n    BS = 10 if arch in ['tf_efficientnet_l2_ns'] else 16\n    train_dataloader = DataLoader(train_dataset, batch_size=BS, num_workers= 2, shuffle=False)\n    \n    with torch.no_grad():\n        res = [model(img.to('cuda')).cpu().numpy() for img in train_dataloader]\n    res = np.concatenate(res, 0)\n    EMB_TEST[arch] = res\n    \n    print( arch, ', Done in:', int(time.time() - starttime), 's' )\n    \n    del model, res\n    torch.cuda.empty_cache() # PyTorch thing to clean RAM\n    gc.collect()\n\nprint(time.time() )    \nlen(EMB_TEST), EMB_TEST.keys()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:46:16.554498Z","iopub.execute_input":"2023-12-05T03:46:16.555273Z","iopub.status.idle":"2023-12-05T03:50:08.105447Z","shell.execute_reply.started":"2023-12-05T03:46:16.555217Z","shell.execute_reply":"2023-12-05T03:50:08.104688Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"deit_base_distilled_patch16_384 , Done in: 19 s\nfbnetc_100 , Done in: 0 s\nig_resnext101_32x8d , Done in: 5 s\nig_resnext101_32x48d , Done in: 41 s\nrepvgg_b0 , Done in: 1 s\nresnetv2_152x4_bitm , Done in: 51 s\nrexnet_200 , Done in: 1 s\nresnest269e , Done in: 9 s\nswsl_resnext101_32x8d , Done in: 5 s\ntf_efficientnet_b6_ns , Done in: 4 s\ntf_efficientnet_b7_ns , Done in: 6 s\ntf_efficientnet_b8_ap , Done in: 8 s\ntf_efficientnet_l2_ns_475 , Done in: 28 s\nvit_base_patch16_384 , Done in: 5 s\nvit_large_patch16_384 , Done in: 17 s\nvit_large_r50_s32_384 , Done in: 18 s\n1701748208.097808\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(16,\n dict_keys(['deit_base_distilled_patch16_384', 'fbnetc_100', 'ig_resnext101_32x8d', 'ig_resnext101_32x48d', 'repvgg_b0', 'resnetv2_152x4_bitm', 'rexnet_200', 'resnest269e', 'swsl_resnext101_32x8d', 'tf_efficientnet_b6_ns', 'tf_efficientnet_b7_ns', 'tf_efficientnet_b8_ap', 'tf_efficientnet_l2_ns_475', 'vit_base_patch16_384', 'vit_large_patch16_384', 'vit_large_r50_s32_384']))"},"metadata":{}}]},{"cell_type":"markdown","source":"# Extract features using Horizontal Flip and small crop","metadata":{}},{"cell_type":"code","source":"class PawpularDataset_HFLIP:\n    def __init__(self, images, base_path='../input/petfinder-pawpularity-score/train/', modelcfg=None, doflip=False ):\n        \n        self.images = images.copy()\n        self.base_path = base_path\n        self.transform = modelcfg\n        self.doflip=doflip\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, item):\n        img = Image.open(self.base_path + self.images[item] + '.jpg').convert('RGB')\n        \n        if self.doflip==True:\n            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n            width, height = img.size\n            img = img.crop((0.0*width, 0.02*height, 0.98*width, 0.98 * height))  \n        \n        img = self.transform(img)\n        return img\n\n\nfor arch in names_hflip_crop:\n    starttime = time.time()\n\n    archname = arch.split('_hflip_')[0]\n    if arch == 'tf_efficientnet_l2_ns_512':\n        archname = 'tf_efficientnet_l2_ns'\n    model = timm.create_model(archname, pretrained=False).to('cuda')\n    model.load_state_dict(torch.load(modelpath[archname]))\n    model.eval()\n\n    # Get model default transforms\n    transf = resolve_data_config({}, model=model)\n    sz = int(arch.split('_')[-1])\n    transf['input_size'] = (3, sz, sz)\n    transf['crop_pct'] = 1.0        \n    transf = create_transform(**transf)\n\n    doflip = True if arch.split('_')[-2] == 'hflip' else False\n    train_dataset = PawpularDataset_HFLIP(\n        images = test.Id.values,\n        base_path='../input/petfinder-pawpularity-score/test/',\n        modelcfg = transf,\n        doflip = doflip,\n    )\n\n    BS = 10 if archname in ['tf_efficientnet_l2_ns'] else 16\n    train_dataloader = DataLoader(train_dataset, batch_size=BS, num_workers= 2, shuffle=False)\n\n    with torch.no_grad():\n        res = [model(img.to('cuda')).cpu().numpy() for img in train_dataloader]\n    res = np.concatenate(res, 0)\n    EMB_TEST[arch] = res\n\n    print( arch, 'imge size:', sz, 'Hflip:', doflip, ',Done in:', int(time.time() - starttime), 's' )\n\n    del model, res\n    torch.cuda.empty_cache() # PyTorch thing to clean RAM\n    gc.collect()\n\nprint(time.time() )    \nlen(EMB_TEST), EMB_TEST.keys()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:50:47.756419Z","iopub.execute_input":"2023-12-05T03:50:47.756727Z","iopub.status.idle":"2023-12-05T03:51:59.847273Z","shell.execute_reply.started":"2023-12-05T03:50:47.756693Z","shell.execute_reply":"2023-12-05T03:51:59.846428Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"tf_efficientnet_l2_ns_hflip_384 imge size: 384 Hflip: True ,Done in: 27 s\ndeit_base_distilled_patch16_384_hflip_384 imge size: 384 Hflip: True ,Done in: 2 s\nig_resnext101_32x48d_hflip_384 imge size: 384 Hflip: True ,Done in: 25 s\ntf_efficientnet_l2_ns_512 imge size: 512 Hflip: False ,Done in: 14 s\n1701748319.8385396\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(20,\n dict_keys(['deit_base_distilled_patch16_384', 'fbnetc_100', 'ig_resnext101_32x8d', 'ig_resnext101_32x48d', 'repvgg_b0', 'resnetv2_152x4_bitm', 'rexnet_200', 'resnest269e', 'swsl_resnext101_32x8d', 'tf_efficientnet_b6_ns', 'tf_efficientnet_b7_ns', 'tf_efficientnet_b8_ap', 'tf_efficientnet_l2_ns_475', 'vit_base_patch16_384', 'vit_large_patch16_384', 'vit_large_r50_s32_384', 'tf_efficientnet_l2_ns_hflip_384', 'deit_base_distilled_patch16_384_hflip_384', 'ig_resnext101_32x48d_hflip_384', 'tf_efficientnet_l2_ns_512']))"},"metadata":{}}]},{"cell_type":"markdown","source":"# Now extract TESTSET features from CLIP architecture","metadata":{}},{"cell_type":"code","source":"class CustomDataset:\n    def __init__(self, data, base_path='../input/petfinder-pawpularity-score/test/', preprocess=None):\n        \n        self.data = data.copy()\n        self.base_path = base_path\n        if 'Pawpularity' not in self.data.columns:\n            self.data['Pawpularity'] = 0\n        self.preprocess=preprocess\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        img = Image.open(self.base_path + self.data.Id[item] + '.jpg').convert(\"RGB\")\n        img = self.preprocess(img)\n        return img\n\n\nfor m in ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B-16', 'ViT-B-32']:\n    starttime = time.time()\n    model, preprocess = clip.load(\"../input/openaiclipweights/clip/CLIP/models/\"+m+\".pt\")\n    model.cuda().eval()\n    \n    EMB = []\n    with torch.no_grad():\n        test_dataset = CustomDataset(data = test, base_path='../input/petfinder-pawpularity-score/test/', preprocess=preprocess)\n        test_data_loader = DataLoader(test_dataset, batch_size=64,num_workers=2,shuffle=False,pin_memory=True,)\n        for batch in test_data_loader:\n            image_features = model.encode_image(batch.to('cuda'))\n            #image_features /= image_features.norm(dim=-1, keepdim=True)\n            logits = image_features.cpu().numpy()\n            EMB.append(logits)\n    EMB = np.concatenate(EMB, 0)\n    EMB = EMB.astype('float32')\n    gc.collect()\n    \n    EMB_TEST['clip_'+m] = EMB\n    print( m, ', Done in:', int(time.time() - starttime), 's' )\n    \n    del model\n    torch.cuda.empty_cache() # PyTorch thing to clean RAM\n    gc.collect()\n    \ngc.collect()\nprint(EMB_TEST.keys())","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:52:29.909177Z","iopub.execute_input":"2023-12-05T03:52:29.909982Z","iopub.status.idle":"2023-12-05T03:53:11.969271Z","shell.execute_reply.started":"2023-12-05T03:52:29.909943Z","shell.execute_reply":"2023-12-05T03:53:11.968492Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"RN50 , Done in: 4 s\nRN101 , Done in: 5 s\nRN50x4 , Done in: 7 s\nRN50x16 , Done in: 11 s\nViT-B-16 , Done in: 6 s\nViT-B-32 , Done in: 5 s\ndict_keys(['deit_base_distilled_patch16_384', 'fbnetc_100', 'ig_resnext101_32x8d', 'ig_resnext101_32x48d', 'repvgg_b0', 'resnetv2_152x4_bitm', 'rexnet_200', 'resnest269e', 'swsl_resnext101_32x8d', 'tf_efficientnet_b6_ns', 'tf_efficientnet_b7_ns', 'tf_efficientnet_b8_ap', 'tf_efficientnet_l2_ns_475', 'vit_base_patch16_384', 'vit_large_patch16_384', 'vit_large_r50_s32_384', 'tf_efficientnet_l2_ns_hflip_384', 'deit_base_distilled_patch16_384_hflip_384', 'ig_resnext101_32x48d_hflip_384', 'tf_efficientnet_l2_ns_512', 'clip_RN50', 'clip_RN101', 'clip_RN50x4', 'clip_RN50x16', 'clip_ViT-B-16', 'clip_ViT-B-32'])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load TRAINSET extracted features (made offline)","metadata":{}},{"cell_type":"code","source":"EMB_TRAIN = joblib.load('../input/petfinderdata/train-embeddings-direct-1.joblib')\ngc.collect()\n\nresclip = joblib.load('../input/openai-clip/train-embeddings-openai-clip-1.joblib')\nfor m in resclip.keys():\n    EMB_TRAIN[m] = resclip[m]\ndel resclip\ngc.collect()\n\nhflipmodels = joblib.load('../input/petfinder-extracted-pretrained-1/extracted-pretrained-1.joblib')\nfor col in names_hflip_crop:\n    EMB_TRAIN[col] = hflipmodels[col]\ndel hflipmodels\ngc.collect()\n\nprint( len(EMB_TRAIN) )\nprint(EMB_TRAIN.keys())","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:53:35.030757Z","iopub.execute_input":"2023-12-05T03:53:35.031338Z","iopub.status.idle":"2023-12-05T03:54:11.083731Z","shell.execute_reply.started":"2023-12-05T03:53:35.031294Z","shell.execute_reply":"2023-12-05T03:54:11.082898Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"58\ndict_keys(['adv_inception_v3', 'cait_xs24_384', 'cspdarknet53', 'deit_base_distilled_patch16_384', 'deit_base_patch16_224', 'dla34', 'dm_nfnet_f2', 'dpn68', 'ese_vovnet19b_dw', 'fbnetc_100', 'hrnet_w18', 'hrnet_w30', 'ig_resnext101_32x8d', 'ig_resnext101_32x48d', 'mixnet_m', 'nfnet_l0', 'pit_ti_distilled_224', 'repvgg_b0', 'resnest101e', 'resnest200e', 'resnest269e', 'resnet34d', 'resnetv2_101x1_bitm', 'resnetv2_152x4_bitm', 'resnetv2_50x1_bitm', 'rexnet_130', 'rexnet_200', 'seresnet50', 'seresnet152d', 'ssl_resnext50_32x4d', 'swin_base_patch4_window7_224_in22k', 'swsl_resnext101_32x4d', 'swsl_resnext101_32x8d', 'tf_efficientnet_b5_ap', 'tf_efficientnet_b8_ap', 'tf_efficientnet_b6_ns', 'tf_efficientnet_b7_ns', 'tf_efficientnet_l2_ns_475', 'tf_efficientnetv2_l_in21ft1k', 'tf_efficientnet_l2_ns', 'tf_mixnet_s', 'twins_pcpvt_small', 'vit_base_patch16_384', 'vit_base_resnet50_384', 'vit_large_patch16_224', 'vit_large_patch16_384', 'vit_large_r50_s32_384', 'xception71', 'clip_RN50', 'clip_RN101', 'clip_RN50x4', 'clip_RN50x16', 'clip_ViT-B-16', 'clip_ViT-B-32', 'tf_efficientnet_l2_ns_hflip_384', 'deit_base_distilled_patch16_384_hflip_384', 'ig_resnext101_32x48d_hflip_384', 'tf_efficientnet_l2_ns_512'])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Explore Heat Map","metadata":{}},{"cell_type":"code","source":"archname = 'swsl_resnext101_32x8d'\nmodel = timm.create_model(archname, pretrained=False).to('cuda')\nmodel.load_state_dict(torch.load(modelpath[archname]))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:59:52.867287Z","iopub.execute_input":"2023-12-05T03:59:52.867674Z","iopub.status.idle":"2023-12-05T03:59:54.616055Z","shell.execute_reply.started":"2023-12-05T03:59:52.867633Z","shell.execute_reply":"2023-12-05T03:59:54.615300Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (act1): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (6): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (7): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (8): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (9): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (10): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (11): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (12): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (13): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (14): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (15): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (16): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (17): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (18): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (19): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (20): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (21): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (22): Bottleneck(\n      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Check the shape of the features","metadata":{}},{"cell_type":"code","source":"for m in EMB_TEST.keys():\n    print(EMB_TRAIN[m].shape, EMB_TEST[m].shape, m)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:16:24.593632Z","iopub.execute_input":"2023-11-30T17:16:24.594424Z","iopub.status.idle":"2023-11-30T17:16:24.611056Z","shell.execute_reply.started":"2023-11-30T17:16:24.594382Z","shell.execute_reply":"2023-11-30T17:16:24.605387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names0 = [\n    'clip_RN50x16',\n    'clip_ViT-B-32',\n    'clip_ViT-B-16',\n    'clip_RN50x4',\n    'deit_base_distilled_patch16_384',\n    'ig_resnext101_32x48d',\n    'repvgg_b0',\n    'resnetv2_152x4_bitm',\n    'swsl_resnext101_32x8d',\n    'tf_efficientnet_l2_ns_475',\n    'vit_base_patch16_384',\n    'vit_large_r50_s32_384',\n]\n\nnames1 = [\n    'clip_RN50x16',\n    'clip_RN101', \n    'clip_RN50',\n    'fbnetc_100',\n    'ig_resnext101_32x8d',\n    'rexnet_200',\n    'resnest269e',\n    'tf_efficientnet_b6_ns',\n    'tf_efficientnet_b8_ap',\n    'tf_efficientnet_b7_ns',\n    'vit_large_patch16_384',\n]\n\nnames2 = [\n    'tf_efficientnet_l2_ns_hflip_384',\n    'deit_base_distilled_patch16_384_hflip_384',\n    'ig_resnext101_32x48d_hflip_384',\n    'tf_efficientnet_l2_ns_512',\n    'ig_resnext101_32x48d',\n    'vit_large_r50_s32_384',\n    'clip_RN50x4',\n    'clip_ViT-B-16',\n    'clip_RN50x16',\n    'clip_ViT-B-32',\n]\n\nnames = np.unique(names0 + names1 + names2)\nlen(names), names","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:16:33.133156Z","iopub.execute_input":"2023-11-30T17:16:33.133481Z","iopub.status.idle":"2023-11-30T17:16:33.145395Z","shell.execute_reply.started":"2023-11-30T17:16:33.133434Z","shell.execute_reply":"2023-11-30T17:16:33.144499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Just clean memory of offline trainset features not going to be used here","metadata":{}},{"cell_type":"code","source":"feats = list(EMB_TRAIN.keys())\nfor n in feats:\n    if n not in names:\n        del EMB_TRAIN[n]\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:16:36.333304Z","iopub.execute_input":"2023-11-30T17:16:36.333630Z","iopub.status.idle":"2023-11-30T17:16:43.770927Z","shell.execute_reply.started":"2023-11-30T17:16:36.333582Z","shell.execute_reply":"2023-11-30T17:16:43.770129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now its time to fit a GPU accelerated SVR using cuml ","metadata":{}},{"cell_type":"code","source":"from cuml.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\n\ndef fit_gpu_svr(TRAIN, TEST, kfoldcol='fold0'):\n    \n    ypredtrain_ = np.zeros(train.shape[0])\n    ypredtest_ = np.zeros(test.shape[0])\n\n    for fold in range(train[kfoldcol].max()+1):\n        ind_train = train[kfoldcol] != fold\n        ind_valid = train[kfoldcol] == fold\n\n        model = SVR(C=16.0, kernel='rbf', degree=3, max_iter=4000, output_type='numpy')\n        model.fit(TRAIN[ind_train], train.Pawpularity[ind_train].clip(1, 85)  )\n\n        ypredtrain_[ind_valid] = np.clip(model.predict(TRAIN[ind_valid]), 1 , 100)\n        ypredtest_ += np.clip(model.predict(TEST), 1, 100)\n\n        del model\n        gc.collect()\n\n    ypredtest_ /= (train[kfoldcol].max()+1)\n\n    return ypredtrain_, ypredtest_\n\ndef rmse(ytrue, ypred):\n    return np.sqrt(np.mean((ytrue-ypred)**2))","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:16:54.383305Z","iopub.execute_input":"2023-11-30T17:16:54.383595Z","iopub.status.idle":"2023-11-30T17:16:54.394333Z","shell.execute_reply.started":"2023-11-30T17:16:54.383566Z","shell.execute_reply":"2023-11-30T17:16:54.393490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First, lets fit one SVR for each architecture independently\n(not used in the submit version)","metadata":{}},{"cell_type":"markdown","source":"for col in names:\n    \n    TRAIN = EMB_TRAIN[col].copy()\n    TEST = EMB_TEST[col].copy()\n\n    scaler = StandardScaler()\n    scaler.fit( np.vstack((TRAIN, TEST)) )\n    TRAIN = scaler.transform(TRAIN)\n    TEST = scaler.transform(TEST)\n    \n    ypredtrain, ypredtest = fit_gpu_svr(TRAIN, TEST, 'fold0')\n    print(rmse(train.Pawpularity,ypredtrain), col)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:42:02.679918Z","iopub.execute_input":"2023-11-30T17:42:02.680222Z","iopub.status.idle":"2023-11-30T17:42:03.472729Z","shell.execute_reply.started":"2023-11-30T17:42:02.680192Z","shell.execute_reply":"2023-11-30T17:42:03.471713Z"}}},{"cell_type":"markdown","source":"As you can see above, features extracted from individual architectures have SVR RMSE ranging from 17.56 to 18.52.","metadata":{}},{"cell_type":"markdown","source":"# Concatenate some features and standardize","metadata":{}},{"cell_type":"markdown","source":"# Fit the SVR A using all K Folds.\n# I noticed cliping the target in 85 slightly boosts RMSE.","metadata":{}},{"cell_type":"code","source":"print('Concatenating:', names0)\n\nTRAIN = np.concatenate([EMB_TRAIN[k] for k in names0], 1)\nTEST = np.concatenate([EMB_TEST[k] for k in names0], 1)\nscaler = StandardScaler()\nscaler.fit( np.vstack((TRAIN, TEST)) )\ngc.collect()\n\nTRAIN = scaler.transform(TRAIN)\nTEST = scaler.transform(TEST)\ngc.collect()\n\n# Check the output shape\nprint(TRAIN.shape, TEST.shape)\n\nypredtrainA, ypredtestA = fit_gpu_svr(TRAIN, TEST, 'fold0')\nprint(rmse(train.Pawpularity, ypredtrainA))","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:26:08.987360Z","iopub.execute_input":"2023-11-30T17:26:08.987697Z","iopub.status.idle":"2023-11-30T17:27:40.135103Z","shell.execute_reply.started":"2023-11-30T17:26:08.987660Z","shell.execute_reply":"2023-11-30T17:27:40.134228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del TRAIN, TEST\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:29:10.345229Z","iopub.execute_input":"2023-11-30T17:29:10.346090Z","iopub.status.idle":"2023-11-30T17:29:10.586171Z","shell.execute_reply.started":"2023-11-30T17:29:10.346051Z","shell.execute_reply":"2023-11-30T17:29:10.585249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Also I noticed that using a multiplier of 1.032 boosts both CV and LB. It may be by the fact that SRV optimizes mean squared error and not RMSE.","metadata":{}},{"cell_type":"code","source":"print('RMSE:', rmse(train.Pawpularity, 1.032*ypredtrainA))","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:29:14.551953Z","iopub.execute_input":"2023-11-30T17:29:14.552277Z","iopub.status.idle":"2023-11-30T17:29:14.559685Z","shell.execute_reply.started":"2023-11-30T17:29:14.552242Z","shell.execute_reply":"2023-11-30T17:29:14.558743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now fit a SVR B using a second subset of features. The idea of fitting more subsets is to add diversity in posterior model ensemble and avoid the curse of dimensionality increasing too much the number of features.","metadata":{}},{"cell_type":"code","source":"print('Concatenating:', names1)\n\nTRAIN = np.concatenate([EMB_TRAIN[k] for k in names1], 1)\nTEST = np.concatenate([EMB_TEST[k] for k in names1], 1)\nscaler = StandardScaler()\nscaler.fit( np.vstack((TRAIN, TEST)) )\ngc.collect()\n\nTRAIN = scaler.transform(TRAIN)\nTEST = scaler.transform(TEST)\ngc.collect()\n\nprint( TRAIN.shape, TEST.shape )\n\nypredtrainB, ypredtestB = fit_gpu_svr(TRAIN, TEST, 'fold0')\nprint('RMSE:', rmse(train.Pawpularity, ypredtrainB))","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:29:17.470107Z","iopub.execute_input":"2023-11-30T17:29:17.470740Z","iopub.status.idle":"2023-11-30T17:30:46.320562Z","shell.execute_reply.started":"2023-11-30T17:29:17.470701Z","shell.execute_reply":"2023-11-30T17:30:46.319589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del TRAIN, TEST\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:32:03.108455Z","iopub.execute_input":"2023-11-30T17:32:03.108812Z","iopub.status.idle":"2023-11-30T17:32:03.347098Z","shell.execute_reply.started":"2023-11-30T17:32:03.108755Z","shell.execute_reply":"2023-11-30T17:32:03.346027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVR C","metadata":{}},{"cell_type":"code","source":"print('Concatenating:', names2)\n\nTRAIN = np.concatenate([EMB_TRAIN[k] for k in names2], 1)\nTEST = np.concatenate([EMB_TEST[k] for k in names2], 1)\nscaler = StandardScaler()\nscaler.fit( np.vstack((TRAIN, TEST)) )\ngc.collect()\n\nTRAIN = scaler.transform(TRAIN)\nTEST = scaler.transform(TEST)\ngc.collect()\n\nprint( TRAIN.shape, TEST.shape )\n\nypredtrainC, ypredtestC = fit_gpu_svr(TRAIN, TEST, 'fold0')\nprint('RMSE:', rmse(train.Pawpularity, ypredtrainC))","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:32:05.097504Z","iopub.execute_input":"2023-11-30T17:32:05.097822Z","iopub.status.idle":"2023-11-30T17:33:20.144846Z","shell.execute_reply.started":"2023-11-30T17:32:05.097768Z","shell.execute_reply":"2023-11-30T17:33:20.143970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del TRAIN, TEST\ndel EMB_TRAIN, EMB_TEST\ngc.collect()\n\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:34:09.601904Z","iopub.execute_input":"2023-11-30T17:34:09.602835Z","iopub.status.idle":"2023-11-30T17:34:10.083411Z","shell.execute_reply.started":"2023-11-30T17:34:09.602766Z","shell.execute_reply":"2023-11-30T17:34:10.082426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimize the overall RMSE using prediction of cuML SVR A, B, and C.","metadata":{}},{"cell_type":"code","source":"from scipy.optimize import minimize\n\ndef min_func(K):\n    ypredtrain = K[0]*ypredtrainA + K[1]*ypredtrainB + K[2]*ypredtrainC\n    return rmse(train.Pawpularity, ypredtrain)\n   \nres = minimize(min_func, [1/3]*3, method='Nelder-Mead', tol=1e-6)\nK = res.x\nres","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:40:56.072389Z","iopub.execute_input":"2023-11-30T17:40:56.072710Z","iopub.status.idle":"2023-11-30T17:40:56.170177Z","shell.execute_reply.started":"2023-11-30T17:40:56.072678Z","shell.execute_reply":"2023-11-30T17:40:56.169246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypredtrain = K[0]*ypredtrainA + K[1]*ypredtrainB + K[2]*ypredtrainC\n\ntest['Pawpularity'] = K[0]*ypredtestA + K[1]*ypredtestB + K[2]*ypredtestC\n\nprint('Ensemble weights:', K )\nprint('Final RMSE:', rmse(train.Pawpularity, ypredtrain) )","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:41:12.073702Z","iopub.execute_input":"2023-11-30T17:41:12.074552Z","iopub.status.idle":"2023-11-30T17:41:12.084306Z","shell.execute_reply.started":"2023-11-30T17:41:12.074482Z","shell.execute_reply":"2023-11-30T17:41:12.083311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(8)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:41:15.056156Z","iopub.execute_input":"2023-11-30T17:41:15.056963Z","iopub.status.idle":"2023-11-30T17:41:15.075684Z","shell.execute_reply.started":"2023-11-30T17:41:15.056920Z","shell.execute_reply":"2023-11-30T17:41:15.074653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[['Id','Pawpularity']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:41:18.729039Z","iopub.execute_input":"2023-11-30T17:41:18.729849Z","iopub.status.idle":"2023-11-30T17:41:18.741042Z","shell.execute_reply.started":"2023-11-30T17:41:18.729773Z","shell.execute_reply":"2023-11-30T17:41:18.740006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}